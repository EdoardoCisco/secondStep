Input Tensor Details:
[{'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 49, 10,  1], dtype=int32), 'shape_signature': array([-1, 49, 10,  1], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.5847029089927673, 83), 'quantization_parameters': {'scales': array([0.5847029], dtype=float32), 'zero_points': array([83], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]

Output Tensor Details:
[{'name': 'StatefulPartitionedCall:0', 'index': 34, 'shape': array([ 1, 12], dtype=int32), 'shape_signature': array([-1, 12], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]

Layer 1: serving_default_input_1:0
Type: <class 'numpy.int8'>
Shape: [ 1 49 10  1]

Layer 2: model/flatten/Const
Type: <class 'numpy.int32'>
Shape: [2]

Layer 3: model/dense/BiasAdd/ReadVariableOp
Type: <class 'numpy.int32'>
Shape: [12]

Layer 4: model/dense/MatMul
Type: <class 'numpy.int8'>
Shape: [12 64]

Layer 5: model/activation_8/Relu;model/batch_normalization_8/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp;model/conv2d_4/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 6: model/conv2d_4/Conv2D
Type: <class 'numpy.int8'>
Shape: [64  1  1 64]

Layer 7: model/activation_7/Relu;model/batch_normalization_7/FusedBatchNormV3;model/depthwise_conv2d_3/depthwise;model/depthwise_conv2d_3/BiasAdd/ReadVariableOp;model/depthwise_conv2d_3/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 8: model/batch_normalization_7/FusedBatchNormV3;model/depthwise_conv2d_3/depthwise;model/depthwise_conv2d_3/BiasAdd/ReadVariableOp;model/depthwise_conv2d_3/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int8'>
Shape: [ 1  3  3 64]

Layer 9: model/activation_6/Relu;model/batch_normalization_6/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp;model/conv2d_3/BiasAdd;model/conv2d_4/Conv2D;model/conv2d_3/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 10: model/conv2d_3/Conv2D
Type: <class 'numpy.int8'>
Shape: [64  1  1 64]

Layer 11: model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/depthwise_conv2d_2/depthwise;model/depthwise_conv2d_2/BiasAdd/ReadVariableOp;model/depthwise_conv2d_2/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 12: model/batch_normalization_5/FusedBatchNormV3;model/depthwise_conv2d_2/depthwise;model/depthwise_conv2d_2/BiasAdd/ReadVariableOp;model/depthwise_conv2d_2/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int8'>
Shape: [ 1  3  3 64]

Layer 13: model/activation_4/Relu;model/batch_normalization_4/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp;model/conv2d_2/BiasAdd;model/conv2d_4/Conv2D;model/conv2d_2/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 14: model/conv2d_2/Conv2D
Type: <class 'numpy.int8'>
Shape: [64  1  1 64]

Layer 15: model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/depthwise_conv2d_1/depthwise;model/depthwise_conv2d_1/BiasAdd/ReadVariableOp;model/depthwise_conv2d_1/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 16: model/batch_normalization_3/FusedBatchNormV3;model/depthwise_conv2d_1/depthwise;model/depthwise_conv2d_1/BiasAdd/ReadVariableOp;model/depthwise_conv2d_1/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int8'>
Shape: [ 1  3  3 64]

Layer 17: model/activation_2/Relu;model/batch_normalization_2/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp;model/conv2d_1/BiasAdd;model/conv2d_4/Conv2D;model/conv2d_1/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 18: model/conv2d_1/Conv2D
Type: <class 'numpy.int8'>
Shape: [64  1  1 64]

Layer 19: model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/depthwise_conv2d/depthwise;model/depthwise_conv2d/BiasAdd/ReadVariableOp;model/depthwise_conv2d/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 20: model/batch_normalization_1/FusedBatchNormV3;model/depthwise_conv2d/depthwise;model/depthwise_conv2d/BiasAdd/ReadVariableOp;model/depthwise_conv2d/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int8'>
Shape: [ 1  3  3 64]

Layer 21: model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp;model/conv2d/BiasAdd;model/conv2d_4/Conv2D;model/conv2d/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 22: model/conv2d/Conv2D
Type: <class 'numpy.int8'>
Shape: [64 10  4  1]

Layer 23: model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp;model/conv2d/BiasAdd;model/conv2d_4/Conv2D;model/conv2d/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 24: model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/depthwise_conv2d/depthwise;model/depthwise_conv2d/BiasAdd/ReadVariableOp;model/depthwise_conv2d/BiasAdd;model/conv2d_4/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 25: model/activation_2/Relu;model/batch_normalization_2/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp;model/conv2d_1/BiasAdd;model/conv2d_4/Conv2D;model/conv2d_1/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 26: model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/depthwise_conv2d_1/depthwise;model/depthwise_conv2d_1/BiasAdd/ReadVariableOp;model/depthwise_conv2d_1/BiasAdd;model/conv2d_4/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 27: model/activation_4/Relu;model/batch_normalization_4/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp;model/conv2d_2/BiasAdd;model/conv2d_4/Conv2D;model/conv2d_2/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 28: model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/depthwise_conv2d_2/depthwise;model/depthwise_conv2d_2/BiasAdd/ReadVariableOp;model/depthwise_conv2d_2/BiasAdd;model/conv2d_4/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 29: model/activation_6/Relu;model/batch_normalization_6/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp;model/conv2d_3/BiasAdd;model/conv2d_4/Conv2D;model/conv2d_3/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 30: model/activation_7/Relu;model/batch_normalization_7/FusedBatchNormV3;model/depthwise_conv2d_3/depthwise;model/depthwise_conv2d_3/BiasAdd/ReadVariableOp;model/depthwise_conv2d_3/BiasAdd;model/conv2d_4/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 31: model/activation_8/Relu;model/batch_normalization_8/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp;model/conv2d_4/BiasAdd;model/conv2d_4/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 32: model/average_pooling2d/AvgPool
Type: <class 'numpy.int8'>
Shape: [ 1  1  1 64]

Layer 33: model/flatten/Reshape
Type: <class 'numpy.int8'>
Shape: [ 1 64]

Layer 34: model/dense/MatMul;model/dense/BiasAdd
Type: <class 'numpy.int8'>
Shape: [ 1 12]

Layer 35: StatefulPartitionedCall:0
Type: <class 'numpy.int8'>
Shape: [ 1 12]

Layer 36: 
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 40]
