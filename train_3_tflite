Input Tensor Details: [{'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 49, 10,  1], dtype=int32), 'shape_signature': array([-1, 49, 10,  1], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.5847029089927673, 83), 'quantization_parameters': {'scales': array([0.5847029], dtype=float32), 'zero_points': array([83], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]

Output Tensor Details: [{'name': 'StatefulPartitionedCall:0', 'index': 34, 'shape': array([ 1, 12], dtype=int32), 'shape_signature': array([-1, 12], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.00390625, -128), 'quantization_parameters': {'scales': array([0.00390625], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]

Layer 1: serving_default_input_1:0
Type: <class 'numpy.int8'>
Shape: [ 1 49 10  1]

Layer 2: model/flatten/Const
Type: <class 'numpy.int32'>
Shape: [2]

Layer 3: model/dense/BiasAdd/ReadVariableOp
Type: <class 'numpy.int32'>
Shape: [12]

Layer 4: model/dense/MatMul
Type: <class 'numpy.int8'>
Shape: [12 64]

Layer 5: model/activation_8/Relu;model/batch_normalization_8/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp;model/conv2d_4/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 6: model/conv2d_4/Conv2D
Type: <class 'numpy.int8'>
Shape: [64  1  1 64]

Layer 7: model/activation_7/Relu;model/batch_normalization_7/FusedBatchNormV3;model/depthwise_conv2d_3/depthwise;model/depthwise_conv2d_3/BiasAdd/ReadVariableOp;model/depthwise_conv2d_3/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 8: model/batch_normalization_7/FusedBatchNormV3;model/depthwise_conv2d_3/depthwise;model/depthwise_conv2d_3/BiasAdd/ReadVariableOp;model/depthwise_conv2d_3/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int8'>
Shape: [ 1  3  3 64]

Layer 9: model/activation_6/Relu;model/batch_normalization_6/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp;model/conv2d_3/BiasAdd;model/conv2d_4/Conv2D;model/conv2d_3/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 10: model/conv2d_3/Conv2D
Type: <class 'numpy.int8'>
Shape: [64  1  1 64]

Layer 11: model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/depthwise_conv2d_2/depthwise;model/depthwise_conv2d_2/BiasAdd/ReadVariableOp;model/depthwise_conv2d_2/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 12: model/batch_normalization_5/FusedBatchNormV3;model/depthwise_conv2d_2/depthwise;model/depthwise_conv2d_2/BiasAdd/ReadVariableOp;model/depthwise_conv2d_2/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int8'>
Shape: [ 1  3  3 64]

Layer 13: model/activation_4/Relu;model/batch_normalization_4/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp;model/conv2d_2/BiasAdd;model/conv2d_4/Conv2D;model/conv2d_2/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 14: model/conv2d_2/Conv2D
Type: <class 'numpy.int8'>
Shape: [64  1  1 64]

Layer 15: model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/depthwise_conv2d_1/depthwise;model/depthwise_conv2d_1/BiasAdd/ReadVariableOp;model/depthwise_conv2d_1/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 16: model/batch_normalization_3/FusedBatchNormV3;model/depthwise_conv2d_1/depthwise;model/depthwise_conv2d_1/BiasAdd/ReadVariableOp;model/depthwise_conv2d_1/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int8'>
Shape: [ 1  3  3 64]

Layer 17: model/activation_2/Relu;model/batch_normalization_2/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp;model/conv2d_1/BiasAdd;model/conv2d_4/Conv2D;model/conv2d_1/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 18: model/conv2d_1/Conv2D
Type: <class 'numpy.int8'>
Shape: [64  1  1 64]

Layer 19: model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/depthwise_conv2d/depthwise;model/depthwise_conv2d/BiasAdd/ReadVariableOp;model/depthwise_conv2d/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 20: model/batch_normalization_1/FusedBatchNormV3;model/depthwise_conv2d/depthwise;model/depthwise_conv2d/BiasAdd/ReadVariableOp;model/depthwise_conv2d/BiasAdd;model/conv2d_4/Conv2D
Type: <class 'numpy.int8'>
Shape: [ 1  3  3 64]

Layer 21: model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp;model/conv2d/BiasAdd;model/conv2d_4/Conv2D;model/conv2d/Conv2D
Type: <class 'numpy.int32'>
Shape: [64]

Layer 22: model/conv2d/Conv2D
Type: <class 'numpy.int8'>
Shape: [64 10  4  1]

Layer 23: model/activation/Relu;model/batch_normalization/FusedBatchNormV3;model/conv2d/BiasAdd/ReadVariableOp;model/conv2d/BiasAdd;model/conv2d_4/Conv2D;model/conv2d/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 24: model/activation_1/Relu;model/batch_normalization_1/FusedBatchNormV3;model/depthwise_conv2d/depthwise;model/depthwise_conv2d/BiasAdd/ReadVariableOp;model/depthwise_conv2d/BiasAdd;model/conv2d_4/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 25: model/activation_2/Relu;model/batch_normalization_2/FusedBatchNormV3;model/conv2d_1/BiasAdd/ReadVariableOp;model/conv2d_1/BiasAdd;model/conv2d_4/Conv2D;model/conv2d_1/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 26: model/activation_3/Relu;model/batch_normalization_3/FusedBatchNormV3;model/depthwise_conv2d_1/depthwise;model/depthwise_conv2d_1/BiasAdd/ReadVariableOp;model/depthwise_conv2d_1/BiasAdd;model/conv2d_4/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 27: model/activation_4/Relu;model/batch_normalization_4/FusedBatchNormV3;model/conv2d_2/BiasAdd/ReadVariableOp;model/conv2d_2/BiasAdd;model/conv2d_4/Conv2D;model/conv2d_2/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 28: model/activation_5/Relu;model/batch_normalization_5/FusedBatchNormV3;model/depthwise_conv2d_2/depthwise;model/depthwise_conv2d_2/BiasAdd/ReadVariableOp;model/depthwise_conv2d_2/BiasAdd;model/conv2d_4/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 29: model/activation_6/Relu;model/batch_normalization_6/FusedBatchNormV3;model/conv2d_3/BiasAdd/ReadVariableOp;model/conv2d_3/BiasAdd;model/conv2d_4/Conv2D;model/conv2d_3/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 30: model/activation_7/Relu;model/batch_normalization_7/FusedBatchNormV3;model/depthwise_conv2d_3/depthwise;model/depthwise_conv2d_3/BiasAdd/ReadVariableOp;model/depthwise_conv2d_3/BiasAdd;model/conv2d_4/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 31: model/activation_8/Relu;model/batch_normalization_8/FusedBatchNormV3;model/conv2d_4/BiasAdd/ReadVariableOp;model/conv2d_4/BiasAdd;model/conv2d_4/Conv2D1
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 64]

Layer 32: model/average_pooling2d/AvgPool
Type: <class 'numpy.int8'>
Shape: [ 1  1  1 64]

Layer 33: model/flatten/Reshape
Type: <class 'numpy.int8'>
Shape: [ 1 64]

Layer 34: model/dense/MatMul;model/dense/BiasAdd
Type: <class 'numpy.int8'>
Shape: [ 1 12]

Layer 35: StatefulPartitionedCall:0
Type: <class 'numpy.int8'>
Shape: [ 1 12]

Layer 36: 
Type: <class 'numpy.int8'>
Shape: [ 1 25  5 40]

=================================================

Layer 0:
Quantization:	(0.5847029089927673, 83)
Scale:	[0.5847029]
Zero point:	[83]

Layer 1:
Quantization:	(0.0, 0)
Scale:	[]
Zero point:	[]

Layer 2:
Quantization:	(0.0007965266122482717, 0)
Scale:	[0.00079653]
Zero point:	[0]

Layer 3:
Quantization:	(0.010367313399910927, 0)
Scale:	[0.01036731]
Zero point:	[0]

Layer 4:
Quantization:	(0.0, 0)
Scale:	[0.00033271 0.00044853 0.00040746 0.00062411 0.00050515 0.00035652
 0.00045517 0.00046245 0.00038656 0.00036056 0.00040242 0.00060075
 0.00039185 0.00041139 0.00058219 0.00056427 0.00045844 0.00039899
 0.00065217 0.0004587  0.0005255  0.00040821 0.0004082  0.00047228
 0.00055612 0.00046819 0.00045248 0.0004629  0.00036587 0.00050678
 0.00035683 0.0003648  0.00054236 0.00040091 0.00049377 0.00040589
 0.00043982 0.00028342 0.00048415 0.0004559  0.00039904 0.00034628
 0.00035113 0.0003407  0.00032672 0.00031168 0.00042251 0.00030553
 0.00042155 0.00069944 0.0003984  0.00059411 0.00041529 0.00049341
 0.00072275 0.00044459 0.00063562 0.00046962 0.00036099 0.00049406
 0.00036718 0.00050057 0.00035287 0.00036098]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 5:
Quantization:	(0.0, 0)
Scale:	[0.00706983 0.00953084 0.00865816 0.01326181 0.01073403 0.00757575
 0.00967205 0.00982662 0.00821402 0.00766164 0.00855107 0.0127654
 0.00832653 0.00874161 0.01237097 0.01199024 0.00974153 0.00847825
 0.01385808 0.00974696 0.01116647 0.00867409 0.00867378 0.01003551
 0.01181705 0.00994867 0.00961476 0.00983626 0.00777446 0.01076867
 0.0075823  0.00775168 0.01152455 0.00851896 0.01049207 0.00862484
 0.0093458  0.00602241 0.01028777 0.00968753 0.0084793  0.00735813
 0.00746113 0.00723958 0.00694257 0.00662294 0.00897805 0.00649227
 0.00895758 0.01486246 0.0084656  0.01262437 0.00882447 0.01048453
 0.01535774 0.00944721 0.0135063  0.00997898 0.00767072 0.01049828
 0.00780222 0.01063663 0.00749822 0.00767048]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 6:
Quantization:	(0.0, 0)
Scale:	[0.00015197 0.00029457 0.00013539 0.00023288 0.00020193 0.00014717
 0.00024666 0.00022379 0.00037493 0.00015105 0.00023124 0.00017332
 0.00017851 0.00024092 0.00017054 0.00030544 0.00020622 0.00017927
 0.00021489 0.00015376 0.00010565 0.00021868 0.00028156 0.00018434
 0.00012461 0.0001398  0.00025557 0.00021912 0.00019684 0.00021
 0.00057677 0.00031209 0.00038836 0.00022398 0.00027884 0.00012918
 0.00022044 0.0002653  0.00024483 0.00023143 0.00023817 0.00015424
 0.00035668 0.00018186 0.00019587 0.00014933 0.00024058 0.00032376
 0.00014427 0.00013592 0.00023463 0.00017704 0.00021914 0.00016791
 0.0002104  0.00014148 0.00028139 0.00020729 0.00025335 0.00021024
 0.00020164 0.00020827 0.00029316 0.00022074]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 7:
Quantization:	(0.0, 0)
Scale:	[0.00492106 0.0095387  0.00438433 0.00754117 0.00653906 0.00476554
 0.0079872  0.00724674 0.01214108 0.00489143 0.00748811 0.00561254
 0.00578048 0.00780134 0.00552233 0.00989084 0.00667782 0.00580508
 0.00695857 0.00497891 0.00342123 0.00708127 0.00911732 0.00596921
 0.00403508 0.00452699 0.00827593 0.00709557 0.00637393 0.00680028
 0.01867684 0.01010606 0.01257599 0.00725305 0.00902956 0.00418298
 0.00713815 0.00859099 0.00792812 0.00749427 0.00771259 0.00499455
 0.01155013 0.00588908 0.00634279 0.00483547 0.00779056 0.01048412
 0.00467186 0.00440121 0.00759775 0.00573282 0.00709616 0.0054373
 0.00681302 0.00458133 0.00911187 0.00671248 0.00820411 0.00680815
 0.00652937 0.00674433 0.00949314 0.00714806]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 8:
Quantization:	(0.0, 0)
Scale:	[0.0002214  0.00014682 0.00016778 0.0001251  0.00015856 0.00018943
 0.00018876 0.00015812 0.00016165 0.00019269 0.00017835 0.00022856
 0.00013536 0.00012123 0.00017348 0.00013486 0.00012158 0.00012859
 0.00015123 0.00014298 0.00019903 0.00012563 0.00011786 0.00015577
 0.00021331 0.00016237 0.00020155 0.00015216 0.00018076 0.00015331
 0.00013323 0.00016819 0.00012058 0.00013424 0.00016175 0.00015712
 0.00019617 0.00014883 0.00015177 0.00018834 0.0001096  0.00018868
 0.00013587 0.00016637 0.00014182 0.00019655 0.00016396 0.00012825
 0.00017408 0.00022001 0.00012632 0.00013101 0.00013706 0.00016037
 0.00015448 0.00016138 0.00015405 0.00016319 0.0001492  0.00014011
 0.00015124 0.00018796 0.00012642 0.0001369 ]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 9:
Quantization:	(0.0, 0)
Scale:	[0.00477615 0.0031673  0.0036195  0.00269872 0.00342051 0.00408645
 0.00407218 0.00341109 0.00348729 0.00415686 0.00384747 0.00493066
 0.00292017 0.00261534 0.00374257 0.0029094  0.00262286 0.0027741
 0.00326241 0.00308445 0.00429366 0.0027101  0.00254259 0.00336044
 0.00460167 0.00350271 0.00434803 0.00328261 0.00389953 0.00330736
 0.00287422 0.00362844 0.0026013  0.00289589 0.00348946 0.00338948
 0.0042319  0.00321075 0.00327419 0.00406312 0.00236442 0.00407043
 0.00293121 0.00358905 0.00305956 0.00424021 0.00353707 0.00276682
 0.0037554  0.00474632 0.00272507 0.00282621 0.00295667 0.00345966
 0.00333255 0.00348137 0.00332336 0.00352058 0.00321864 0.00302248
 0.0032627  0.00405488 0.00272729 0.00295329]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 10:
Quantization:	(0.0, 0)
Scale:	[0.00030948 0.00036395 0.00025136 0.00024486 0.00016168 0.0002084
 0.00033492 0.00042146 0.0003149  0.00020834 0.00025345 0.00026961
 0.00031013 0.00037079 0.0001901  0.00026131 0.00030964 0.00024895
 0.00033285 0.00023346 0.00032036 0.00023035 0.00024731 0.00030219
 0.0003129  0.00017875 0.00030739 0.00039706 0.00036662 0.00022983
 0.00030125 0.00024352 0.00033688 0.000329   0.00023307 0.00024817
 0.00030521 0.00033307 0.00022068 0.00025924 0.00024999 0.00020649
 0.00031711 0.00027729 0.00030871 0.00030671 0.00022391 0.00035362
 0.00035404 0.00025412 0.00040341 0.00028809 0.00023788 0.00022533
 0.00023795 0.00028329 0.00036722 0.00034949 0.00042215 0.00020698
 0.00016912 0.00030309 0.0001957  0.0001652 ]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 11:
Quantization:	(0.0, 0)
Scale:	[0.00814319 0.0095766  0.00661408 0.00644304 0.0042543  0.00548359
 0.00881277 0.01108965 0.00828578 0.00548207 0.00666908 0.00709421
 0.00816025 0.00975644 0.00500214 0.00687576 0.00814744 0.00655061
 0.00875821 0.0061431  0.00842958 0.00606102 0.00650737 0.00795145
 0.00823327 0.00470332 0.00808825 0.01044767 0.00964683 0.00604754
 0.00792663 0.00640762 0.00886426 0.00865688 0.0061328  0.00653008
 0.00803086 0.00876392 0.00580676 0.00682127 0.00657789 0.0054333
 0.00834394 0.00729635 0.00812308 0.00807051 0.0058916  0.00930469
 0.00931566 0.00668649 0.01061487 0.00758031 0.00625925 0.00592895
 0.00626111 0.00745425 0.00966244 0.00919616 0.01110802 0.00544613
 0.00445003 0.00797517 0.00514947 0.0043468 ]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 12:
Quantization:	(0.0, 0)
Scale:	[0.00025523 0.00028637 0.00021125 0.00028174 0.00027708 0.00021754
 0.00019371 0.00022686 0.00017489 0.00030173 0.00025674 0.00030272
 0.00029106 0.00015201 0.00024678 0.00019943 0.00032496 0.00026898
 0.0002541  0.00031535 0.00034596 0.00021599 0.00035508 0.00019404
 0.00022734 0.00024558 0.00019563 0.00016255 0.00017995 0.00028231
 0.0002001  0.00028407 0.00019746 0.00019312 0.00023057 0.00026416
 0.00018035 0.00023292 0.00024339 0.00019597 0.00022351 0.00019927
 0.00023466 0.00030694 0.00026994 0.00027159 0.00026981 0.0001974
 0.00017631 0.0002586  0.0002123  0.0001938  0.00028472 0.00027698
 0.00023213 0.00028216 0.00023009 0.00016762 0.00022984 0.00023242
 0.00026999 0.00023056 0.00030764 0.00029999]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 13:
Quantization:	(0.0, 0)
Scale:	[0.00344574 0.00386623 0.00285203 0.00380372 0.00374078 0.00293694
 0.00261523 0.00306272 0.00236114 0.00407357 0.00346619 0.00408685
 0.00392945 0.00205225 0.0033317  0.00269247 0.00438721 0.00363142
 0.00343046 0.00425738 0.00467064 0.00291599 0.00479377 0.00261971
 0.00306925 0.00331548 0.00264111 0.00219455 0.00242937 0.00381137
 0.00270145 0.00383507 0.00266587 0.00260717 0.00311289 0.00356629
 0.00243478 0.00314452 0.00328587 0.00264576 0.00301749 0.00269022
 0.003168   0.00414393 0.00364439 0.00366657 0.00364256 0.00266502
 0.00238032 0.00349131 0.00286618 0.00261638 0.0038439  0.00373937
 0.00313386 0.0038093  0.00310635 0.00226295 0.00310292 0.00313784
 0.003645   0.00311276 0.00415329 0.00404997]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 14:
Quantization:	(0.0, 0)
Scale:	[0.00042957 0.00041249 0.00032802 0.00052763 0.00056473 0.00075485
 0.00059749 0.00034387 0.00046393 0.0003385  0.00030324 0.00048759
 0.00027429 0.00038403 0.00055239 0.00078971 0.00061007 0.00057419
 0.00053215 0.0003065  0.00041928 0.00043014 0.000422   0.00051594
 0.0003907  0.00052294 0.00053866 0.00039658 0.00030227 0.00041208
 0.00040923 0.00046842 0.00029305 0.00059866 0.00049159 0.00044799
 0.000775   0.0002745  0.00072269 0.00041429 0.00041426 0.00055011
 0.00045964 0.00064858 0.0002986  0.00089741 0.00035858 0.00052416
 0.00027961 0.00037743 0.00074837 0.00070458 0.00032419 0.00039285
 0.00062001 0.0003748  0.00042483 0.00059341 0.00040305 0.00047518
 0.00031728 0.00033984 0.00057912 0.00036824]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 15:
Quantization:	(0.0, 0)
Scale:	[0.00767146 0.00736649 0.00585795 0.00942268 0.01008523 0.01348052
 0.01067033 0.00614103 0.00828523 0.00604521 0.00541552 0.00870765
 0.00489851 0.00685829 0.00986489 0.01410307 0.01089497 0.0102542
 0.00950355 0.00547359 0.00748774 0.0076818  0.00753629 0.00921403
 0.00697744 0.00933902 0.0096198  0.00708237 0.00539812 0.00735919
 0.00730824 0.00836528 0.0052335  0.01069126 0.00877919 0.00800045
 0.01384053 0.0049022  0.01290627 0.00739871 0.00739811 0.00982429
 0.00820851 0.01158283 0.0053326  0.01602653 0.00640376 0.00936071
 0.0049935  0.00674036 0.01336496 0.01258276 0.0057896  0.00701585
 0.01107263 0.00669334 0.00758689 0.01059757 0.00719801 0.00848613
 0.00566622 0.00606902 0.01034222 0.00657623]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 16:
Quantization:	(0.0, 0)
Scale:	[0.00018963 0.00028498 0.00023251 0.00030838 0.00025209 0.00025674
 0.00021886 0.00025847 0.0002392  0.00019853 0.00025168 0.00021849
 0.00033378 0.00021684 0.00030348 0.00025892 0.00028486 0.0001921
 0.00021031 0.00022449 0.00021624 0.00021965 0.00021648 0.00027732
 0.00015867 0.00027672 0.00024406 0.00020685 0.00024974 0.00035039
 0.000368   0.000179   0.00030164 0.00022196 0.00022677 0.00021654
 0.00023628 0.00027918 0.00017662 0.00024853 0.00022123 0.00036708
 0.0002619  0.00018369 0.00021561 0.00015839 0.00023639 0.00033372
 0.00018911 0.00026368 0.00015618 0.00018628 0.00018747 0.00019214
 0.00028277 0.00023049 0.00037888 0.00026606 0.00029937 0.000211
 0.00027861 0.00019546 0.00021553 0.00023534]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 17:
Quantization:	(0.0, 0)
Scale:	[0.00226186 0.00339927 0.00277331 0.00367835 0.00300686 0.00306242
 0.00261053 0.00308298 0.00285322 0.00236804 0.00300206 0.00260619
 0.00398132 0.00258644 0.00361986 0.00308839 0.00339774 0.00229141
 0.00250857 0.00267771 0.00257928 0.00262    0.00258211 0.00330784
 0.00189267 0.00330074 0.00291114 0.0024673  0.0029789  0.0041794
 0.00438948 0.00213515 0.00359799 0.00264758 0.00270486 0.00258283
 0.00281837 0.00333007 0.00210674 0.00296447 0.00263884 0.00437855
 0.00312396 0.00219102 0.00257175 0.00188923 0.0028197  0.00398059
 0.00225564 0.00314513 0.00186295 0.00222195 0.00223614 0.00229182
 0.00337284 0.00274932 0.00451928 0.0031735  0.0035709  0.00251676
 0.00332322 0.00233148 0.00257078 0.00280715]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 18:
Quantization:	(0.0, 0)
Scale:	[0.00138639 0.00065056 0.0006851  0.00153551 0.00155727 0.00042076
 0.00060958 0.00182755 0.0012884  0.00112428 0.00109681 0.00227195
 0.00094908 0.00092957 0.00246004 0.0006036  0.0011475  0.00089336
 0.00208999 0.00096005 0.00044736 0.00134743 0.00154548 0.00076638
 0.00099436 0.00174987 0.00278898 0.00105503 0.0017024  0.00046775
 0.00060073 0.0008221  0.00110936 0.00125966 0.00071939 0.00101774
 0.00120681 0.00086323 0.00138156 0.0008145  0.00083424 0.00054531
 0.00189114 0.0005998  0.00060843 0.00067732 0.00057182 0.00077941
 0.00115481 0.00049712 0.00044329 0.00136316 0.00117434 0.00152202
 0.00064612 0.0004337  0.00042769 0.00088745 0.0006282  0.0011308
 0.00062548 0.00160257 0.00050727 0.00047282]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 19:
Quantization:	(0.0, 0)
Scale:	[0.01511223 0.00709141 0.00746787 0.01673772 0.01697496 0.00458651
 0.00664466 0.01992114 0.01404417 0.01225511 0.01195574 0.02476523
 0.01034539 0.01013275 0.0268155  0.00657946 0.01250823 0.00973798
 0.02278179 0.01046494 0.00487645 0.01468763 0.01684644 0.0083539
 0.01083898 0.01907438 0.0304011  0.01150034 0.01855686 0.00509864
 0.00654825 0.00896124 0.01209247 0.01373082 0.00784162 0.01109378
 0.01315472 0.00940963 0.01505961 0.00887843 0.00909356 0.00594412
 0.0206143  0.00653807 0.00663211 0.00738306 0.00623304 0.00849592
 0.01258799 0.00541883 0.00483211 0.01485904 0.01280081 0.01659066
 0.00704302 0.00472754 0.00466204 0.00967361 0.0068477  0.01232623
 0.00681797 0.01746868 0.00552947 0.00515395]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 20:
Quantization:	(0.0, 0)
Scale:	[5.77432569e-04 4.50392137e-04 1.82232194e-04 3.74854280e-04
 3.72164912e-04 3.17426457e-04 5.65559021e-04 2.47078133e-04
 8.19933077e-04 6.80108438e-04 4.37148672e-04 1.95779547e-04
 1.04903811e-04 7.22512428e-04 4.67592617e-04 2.02797120e-04
 4.37242008e-04 5.60475281e-04 2.59661174e-04 1.46092891e-04
 2.07695339e-04 4.92959341e-04 4.82563511e-04 3.44446365e-04
 2.83229834e-04 2.39221263e-04 9.44320200e-05 7.24012149e-04
 4.81388095e-04 3.17798287e-04 3.88763932e-04 1.26895483e-03
 7.39164010e-04 2.82163732e-04 6.28587441e-04 5.89473289e-04
 4.82550735e-04 1.16932875e-04 6.02999993e-04 6.27158734e-04
 5.21327427e-04 2.49447097e-04 3.18631966e-04 1.54640875e-04
 2.69935845e-04 4.05027939e-04 3.48469912e-04 5.82330162e-04
 7.30681466e-04 5.59689710e-04 4.15832852e-04 2.70405231e-04
 3.82715487e-04 6.59781450e-04 3.16374062e-04 3.01262800e-04
 5.63212379e-04 4.51099681e-04 4.76073008e-04 1.71194610e-04
 4.13957488e-04 4.38903837e-04 1.62577460e-04 2.34969062e-04]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 21:
Quantization:	(0.0, 0)
Scale:	[0.00098757 0.00077029 0.00031167 0.0006411  0.0006365  0.00054289
 0.00096726 0.00042257 0.00140231 0.00116317 0.00074764 0.00033484
 0.00017941 0.00123569 0.00079971 0.00034684 0.0007478  0.00095856
 0.00044409 0.00024986 0.00035522 0.00084309 0.00082531 0.0005891
 0.0004844  0.00040913 0.0001615  0.00123826 0.0008233  0.00054352
 0.00066489 0.00217026 0.00126417 0.00048258 0.00107505 0.00100816
 0.00082529 0.00019999 0.00103129 0.00107261 0.00089161 0.00042662
 0.00054495 0.00026448 0.00046166 0.00069271 0.00059598 0.00099594
 0.00124966 0.00095722 0.00071119 0.00046247 0.00065455 0.0011284
 0.00054109 0.00051524 0.00096325 0.0007715  0.00081421 0.00029279
 0.00070798 0.00075064 0.00027805 0.00040186]
Zero point:	[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

Layer 22:
Quantization:	(0.09173938632011414, -128)
Scale:	[0.09173939]
Zero point:	[-128]

Layer 23:
Quantization:	(0.0838366374373436, -128)
Scale:	[0.08383664]
Zero point:	[-128]

Layer 24:
Quantization:	(0.05599527806043625, -128)
Scale:	[0.05599528]
Zero point:	[-128]

Layer 25:
Quantization:	(0.07407091557979584, -128)
Scale:	[0.07407092]
Zero point:	[-128]

Layer 26:
Quantization:	(0.03800439089536667, -128)
Scale:	[0.03800439]
Zero point:	[-128]

Layer 27:
Quantization:	(0.04635448008775711, -128)
Scale:	[0.04635448]
Zero point:	[-128]

Layer 28:
Quantization:	(0.03088134154677391, -128)
Scale:	[0.03088134]
Zero point:	[-128]

Layer 29:
Quantization:	(0.04706083610653877, -128)
Scale:	[0.04706084]
Zero point:	[-128]

Layer 30:
Quantization:	(0.07683057337999344, -128)
Scale:	[0.07683057]
Zero point:	[-128]

Layer 31:
Quantization:	(0.07683057337999344, -128)
Scale:	[0.07683057]
Zero point:	[-128]

Layer 32:
Quantization:	(0.07683057337999344, -128)
Scale:	[0.07683057]
Zero point:	[-128]

Layer 33:
Quantization:	(0.18429847061634064, 24)
Scale:	[0.18429847]
Zero point:	[24]

Layer 34:
Quantization:	(0.00390625, -128)
Scale:	[0.00390625]
Zero point:	[-128]

Layer 35:
Quantization:	(0.0, 0)
Scale:	[]
Zero point:	[]

